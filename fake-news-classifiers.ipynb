{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The aim of the project is to build a fake news classifier using Natural Language Processing.","metadata":{}},{"cell_type":"code","source":"## Loading necessary libraries\nimport nltk\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:23:54.943123Z","iopub.execute_input":"2021-10-31T17:23:54.943874Z","iopub.status.idle":"2021-10-31T17:23:56.653250Z","shell.execute_reply.started":"2021-10-31T17:23:54.943814Z","shell.execute_reply":"2021-10-31T17:23:56.652452Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## Reading the data\ndf=pd.read_csv('../input/fake-news-classifier/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:14.502307Z","iopub.execute_input":"2021-10-31T17:24:14.503140Z","iopub.status.idle":"2021-10-31T17:24:17.265067Z","shell.execute_reply.started":"2021-10-31T17:24:14.503099Z","shell.execute_reply":"2021-10-31T17:24:17.264326Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:18.216841Z","iopub.execute_input":"2021-10-31T17:24:18.217120Z","iopub.status.idle":"2021-10-31T17:24:18.244939Z","shell.execute_reply.started":"2021-10-31T17:24:18.217090Z","shell.execute_reply":"2021-10-31T17:24:18.244248Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.shape\n## There are 20800 rows and 5 columns as seen above","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:24.703517Z","iopub.execute_input":"2021-10-31T17:24:24.704310Z","iopub.status.idle":"2021-10-31T17:24:24.710167Z","shell.execute_reply.started":"2021-10-31T17:24:24.704270Z","shell.execute_reply":"2021-10-31T17:24:24.709323Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\n## Increasing the width of the the columns","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:29.561875Z","iopub.execute_input":"2021-10-31T17:24:29.562149Z","iopub.status.idle":"2021-10-31T17:24:29.566026Z","shell.execute_reply.started":"2021-10-31T17:24:29.562119Z","shell.execute_reply":"2021-10-31T17:24:29.565267Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df['title']\n## Title contains the headline of the news","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:32.442404Z","iopub.execute_input":"2021-10-31T17:24:32.442843Z","iopub.status.idle":"2021-10-31T17:24:32.453509Z","shell.execute_reply.started":"2021-10-31T17:24:32.442801Z","shell.execute_reply":"2021-10-31T17:24:32.452483Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df['text']\n## text contains the information regarding the headline.","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:37.292023Z","iopub.execute_input":"2021-10-31T17:24:37.292319Z","iopub.status.idle":"2021-10-31T17:24:37.314281Z","shell.execute_reply.started":"2021-10-31T17:24:37.292286Z","shell.execute_reply":"2021-10-31T17:24:37.313355Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df['label'].value_counts()\n## There are 10413 'ones' and 10387 'zeroes' in the dataframe","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:48.848403Z","iopub.execute_input":"2021-10-31T17:24:48.849190Z","iopub.status.idle":"2021-10-31T17:24:48.859142Z","shell.execute_reply.started":"2021-10-31T17:24:48.849148Z","shell.execute_reply":"2021-10-31T17:24:48.858326Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()\n## There are few null values present in the dataframe","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:51.928465Z","iopub.execute_input":"2021-10-31T17:24:51.928787Z","iopub.status.idle":"2021-10-31T17:24:51.947896Z","shell.execute_reply.started":"2021-10-31T17:24:51.928751Z","shell.execute_reply":"2021-10-31T17:24:51.947175Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df=df.dropna()\n## The null values are removed using the dropna function","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:55.537165Z","iopub.execute_input":"2021-10-31T17:24:55.537982Z","iopub.status.idle":"2021-10-31T17:24:55.556569Z","shell.execute_reply.started":"2021-10-31T17:24:55.537936Z","shell.execute_reply":"2021-10-31T17:24:55.555728Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()\n## As seen below there are no null values present in the dataframe now.","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:24:58.257102Z","iopub.execute_input":"2021-10-31T17:24:58.257904Z","iopub.status.idle":"2021-10-31T17:24:58.273510Z","shell.execute_reply.started":"2021-10-31T17:24:58.257867Z","shell.execute_reply":"2021-10-31T17:24:58.272595Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df.reset_index(inplace=True)\n## As we can see in the output, the Series. reset_index() function has reset the index of the given Series.","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:25:02.302024Z","iopub.execute_input":"2021-10-31T17:25:02.302326Z","iopub.status.idle":"2021-10-31T17:25:02.309084Z","shell.execute_reply.started":"2021-10-31T17:25:02.302294Z","shell.execute_reply":"2021-10-31T17:25:02.308071Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:25:07.077297Z","iopub.execute_input":"2021-10-31T17:25:07.078323Z","iopub.status.idle":"2021-10-31T17:25:07.101520Z","shell.execute_reply.started":"2021-10-31T17:25:07.078268Z","shell.execute_reply":"2021-10-31T17:25:07.100635Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#df=df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:25:25.402307Z","iopub.execute_input":"2021-10-31T17:25:25.403170Z","iopub.status.idle":"2021-10-31T17:25:25.407244Z","shell.execute_reply.started":"2021-10-31T17:25:25.403122Z","shell.execute_reply":"2021-10-31T17:25:25.406235Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import re\nimport string","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:25:28.101996Z","iopub.execute_input":"2021-10-31T17:25:28.102535Z","iopub.status.idle":"2021-10-31T17:25:28.105913Z","shell.execute_reply.started":"2021-10-31T17:25:28.102498Z","shell.execute_reply":"2021-10-31T17:25:28.105110Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Text cleaning","metadata":{}},{"cell_type":"code","source":"# remove all numbers with letters attached to them\nalphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n\n# .lower() - convert all strings to lowercase \npunc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n\n# Remove all '\\n' in the string and replace it with a space\nremove_n = lambda x: re.sub(\"\\n\", \" \", x)\n\n# Remove all non-ascii characters \nremove_non_ascii = lambda x: re.sub(r'[^\\x00-\\x7f]',r' ', x)\n\n# Apply all the lambda functions wrote previously through .map on the comments column\ndf['text'] = df['text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:25:32.342270Z","iopub.execute_input":"2021-10-31T17:25:32.342550Z","iopub.status.idle":"2021-10-31T17:26:02.148077Z","shell.execute_reply.started":"2021-10-31T17:25:32.342517Z","shell.execute_reply":"2021-10-31T17:26:02.146924Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df['text']","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:26:15.541529Z","iopub.execute_input":"2021-10-31T17:26:15.542292Z","iopub.status.idle":"2021-10-31T17:26:15.554465Z","shell.execute_reply.started":"2021-10-31T17:26:15.542253Z","shell.execute_reply":"2021-10-31T17:26:15.553703Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Removing stop words and stemming the text","metadata":{}},{"cell_type":"markdown","source":"#### In natural language processing, useless words (data), are referred to as stop words. ... Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.","metadata":{}},{"cell_type":"markdown","source":"#### The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:26:32.262509Z","iopub.execute_input":"2021-10-31T17:26:32.263336Z","iopub.status.idle":"2021-10-31T17:26:32.506499Z","shell.execute_reply.started":"2021-10-31T17:26:32.263301Z","shell.execute_reply":"2021-10-31T17:26:32.505685Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:30:47.198294Z","iopub.execute_input":"2021-10-31T17:30:47.198897Z","iopub.status.idle":"2021-10-31T17:30:47.202632Z","shell.execute_reply.started":"2021-10-31T17:30:47.198859Z","shell.execute_reply":"2021-10-31T17:30:47.201723Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\nps = PorterStemmer()\ncorpus = []\nfor i in tqdm(range(0, len(df))):\n    review = re.sub('[^a-zA-Z]', ' ', df['text'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T17:31:21.453080Z","iopub.execute_input":"2021-10-31T17:31:21.453367Z","iopub.status.idle":"2021-10-31T18:09:08.703864Z","shell.execute_reply.started":"2021-10-31T17:31:21.453335Z","shell.execute_reply":"2021-10-31T18:09:08.703005Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the dataframe","metadata":{}},{"cell_type":"code","source":"Y=df['label']\n## We select the label column as Y","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:09:17.175805Z","iopub.execute_input":"2021-10-31T18:09:17.176088Z","iopub.status.idle":"2021-10-31T18:09:17.180808Z","shell.execute_reply.started":"2021-10-31T18:09:17.176052Z","shell.execute_reply":"2021-10-31T18:09:17.180165Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"Y.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-31T18:09:20.899929Z","iopub.execute_input":"2021-10-31T18:09:20.900385Z","iopub.status.idle":"2021-10-31T18:09:20.907400Z","shell.execute_reply.started":"2021-10-31T18:09:20.900333Z","shell.execute_reply":"2021-10-31T18:09:20.906823Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df.to_csv('cleaned_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:15:55.547712Z","iopub.execute_input":"2021-10-31T18:15:55.548022Z","iopub.status.idle":"2021-10-31T18:15:59.311106Z","shell.execute_reply.started":"2021-10-31T18:15:55.547989Z","shell.execute_reply":"2021-10-31T18:15:59.310172Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Making train and test data","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(df['text'], Y, test_size=0.30, random_state=40)\n## We have split the data into 70 percent train and 30 percent test","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:09:25.409219Z","iopub.execute_input":"2021-10-31T18:09:25.409491Z","iopub.status.idle":"2021-10-31T18:09:25.422423Z","shell.execute_reply.started":"2021-10-31T18:09:25.409462Z","shell.execute_reply":"2021-10-31T18:09:25.421390Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Tfidf vectorizer","metadata":{}},{"cell_type":"markdown","source":"Understanding TfidfVectorizer Using a Simple Example\nThe TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.","metadata":{}},{"cell_type":"code","source":"#Applying tfidf to the data set\ntfidf_vect = TfidfVectorizer(stop_words = 'english',max_df=0.7)\ntfidf_train = tfidf_vect.fit_transform(X_train)\ntfidf_test = tfidf_vect.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:16:23.340948Z","iopub.execute_input":"2021-10-31T18:16:23.341229Z","iopub.status.idle":"2021-10-31T18:16:38.731514Z","shell.execute_reply.started":"2021-10-31T18:16:23.341201Z","shell.execute_reply":"2021-10-31T18:16:38.730810Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(tfidf_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:16:38.732855Z","iopub.execute_input":"2021-10-31T18:16:38.733706Z","iopub.status.idle":"2021-10-31T18:16:38.750650Z","shell.execute_reply.started":"2021-10-31T18:16:38.733664Z","shell.execute_reply":"2021-10-31T18:16:38.749995Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Get the feature names of `tfidf_vectorizer` \nprint(tfidf_vect.get_feature_names()[-10:])","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:16:57.416677Z","iopub.execute_input":"2021-10-31T18:16:57.417589Z","iopub.status.idle":"2021-10-31T18:16:57.549277Z","shell.execute_reply.started":"2021-10-31T18:16:57.417533Z","shell.execute_reply":"2021-10-31T18:16:57.548303Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Count vectorizer","metadata":{}},{"cell_type":"markdown","source":"Understanding CountVectorizer\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\nYou can use it as follows:\n\nCreate an instance of the CountVectorizer class.\nCall the fit() function in order to learn a vocabulary from one or more documents.\nCall the transform() function on one or more documents as needed to encode each as a vector.\nAn encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.","metadata":{}},{"cell_type":"code","source":"count_vect = CountVectorizer(stop_words = 'english')\ncount_train = count_vect.fit_transform(X_train.values)\ncount_test = count_vect.transform(X_test.values)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:17:05.744760Z","iopub.execute_input":"2021-10-31T18:17:05.745062Z","iopub.status.idle":"2021-10-31T18:17:20.737760Z","shell.execute_reply.started":"2021-10-31T18:17:05.745028Z","shell.execute_reply":"2021-10-31T18:17:20.737054Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"print(count_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:17:20.739504Z","iopub.execute_input":"2021-10-31T18:17:20.739804Z","iopub.status.idle":"2021-10-31T18:17:20.758172Z","shell.execute_reply.started":"2021-10-31T18:17:20.739770Z","shell.execute_reply":"2021-10-31T18:17:20.757128Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Get the feature names of `count_vectorizer` \nprint(count_vect.get_feature_names()[-10:])","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:18:27.174648Z","iopub.execute_input":"2021-10-31T18:18:27.174967Z","iopub.status.idle":"2021-10-31T18:18:27.311416Z","shell.execute_reply.started":"2021-10-31T18:18:27.174931Z","shell.execute_reply":"2021-10-31T18:18:27.310491Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Machine learning","metadata":{}},{"cell_type":"markdown","source":"# 1. Naive Bayes model","metadata":{}},{"cell_type":"markdown","source":"### TF-Idf vectorized ","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:18:42.844578Z","iopub.execute_input":"2021-10-31T18:18:42.844867Z","iopub.status.idle":"2021-10-31T18:18:42.851250Z","shell.execute_reply.started":"2021-10-31T18:18:42.844839Z","shell.execute_reply":"2021-10-31T18:18:42.850527Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:18:48.299045Z","iopub.execute_input":"2021-10-31T18:18:48.299330Z","iopub.status.idle":"2021-10-31T18:18:48.302977Z","shell.execute_reply.started":"2021-10-31T18:18:48.299300Z","shell.execute_reply":"2021-10-31T18:18:48.302195Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#Applying Naive Bayes\nclf = MultinomialNB() \nclf.fit(tfidf_train, Y_train)                       \npred = clf.predict(tfidf_test)                     \nscore = metrics.accuracy_score(Y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(Y_test, pred)\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:18:51.411030Z","iopub.execute_input":"2021-10-31T18:18:51.411440Z","iopub.status.idle":"2021-10-31T18:18:51.479180Z","shell.execute_reply.started":"2021-10-31T18:18:51.411409Z","shell.execute_reply":"2021-10-31T18:18:51.478245Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print('Wrong predictions out of total')\nprint((Y_test !=pred).sum(),'/',((Y_test == pred).sum()+(Y_test != pred).sum()))\nprint('Percentage accuracy: ',100*accuracy_score(Y_test,pred))","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:18:56.469608Z","iopub.execute_input":"2021-10-31T18:18:56.470352Z","iopub.status.idle":"2021-10-31T18:18:56.478980Z","shell.execute_reply.started":"2021-10-31T18:18:56.470312Z","shell.execute_reply":"2021-10-31T18:18:56.478211Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"## Plotting confusion matrix for TF-Idf vectorizer","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:19:01.386616Z","iopub.execute_input":"2021-10-31T18:19:01.387384Z","iopub.status.idle":"2021-10-31T18:19:01.391545Z","shell.execute_reply.started":"2021-10-31T18:19:01.387346Z","shell.execute_reply":"2021-10-31T18:19:01.390657Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(cm, cmap=\"plasma\", annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:19:04.623773Z","iopub.execute_input":"2021-10-31T18:19:04.624190Z","iopub.status.idle":"2021-10-31T18:19:04.920147Z","shell.execute_reply.started":"2021-10-31T18:19:04.624157Z","shell.execute_reply":"2021-10-31T18:19:04.919298Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### Count Vectorized","metadata":{}},{"cell_type":"code","source":"#Applying Naive Bayes\nclf = MultinomialNB() \nclf.fit(count_train, Y_train)                       \npred1 = clf.predict(count_test)                    \nscore = metrics.accuracy_score(Y_test, pred1)\nprint(\"accuracy:   %0.3f\" % score)\ncm2 = metrics.confusion_matrix(Y_test, pred1)\nprint(cm2)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:19:25.087891Z","iopub.execute_input":"2021-10-31T18:19:25.088184Z","iopub.status.idle":"2021-10-31T18:19:25.151683Z","shell.execute_reply.started":"2021-10-31T18:19:25.088150Z","shell.execute_reply":"2021-10-31T18:19:25.150765Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"print('Wrong predictions out of total')\nprint((Y_test !=pred1).sum(),'/',((Y_test == pred1).sum()+(Y_test != pred1).sum()))\nprint('Percentage accuracy: ',100*accuracy_score(Y_test,pred1))","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:19:28.811914Z","iopub.execute_input":"2021-10-31T18:19:28.812505Z","iopub.status.idle":"2021-10-31T18:19:28.822364Z","shell.execute_reply.started":"2021-10-31T18:19:28.812464Z","shell.execute_reply":"2021-10-31T18:19:28.821321Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"## Plotting confusion matrix for Count vectorizer.","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:19:32.936861Z","iopub.execute_input":"2021-10-31T18:19:32.937152Z","iopub.status.idle":"2021-10-31T18:19:32.940566Z","shell.execute_reply.started":"2021-10-31T18:19:32.937117Z","shell.execute_reply":"2021-10-31T18:19:32.939960Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(cm2, cmap=\"plasma\", annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:19:35.699415Z","iopub.execute_input":"2021-10-31T18:19:35.700065Z","iopub.status.idle":"2021-10-31T18:19:35.947923Z","shell.execute_reply.started":"2021-10-31T18:19:35.700023Z","shell.execute_reply":"2021-10-31T18:19:35.947294Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# 2. Random Forest Model","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:19:41.396715Z","iopub.execute_input":"2021-10-31T18:19:41.397236Z","iopub.status.idle":"2021-10-31T18:19:41.507101Z","shell.execute_reply.started":"2021-10-31T18:19:41.397199Z","shell.execute_reply":"2021-10-31T18:19:41.506352Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### TF-Idf Vectorized","metadata":{}},{"cell_type":"code","source":"RF=RandomForestClassifier().fit(tfidf_train,Y_train)\n#predict on train \ntrain_preds2 = RF.predict(tfidf_train)\n#accuracy on train\nprint(\"Model accuracy on train is: \", accuracy_score(Y_train, train_preds2))\n\n#predict on test\ntest_preds2 = RF.predict(tfidf_test)\n#accuracy on test\nprint(\"Model accuracy on test is: \", accuracy_score(Y_test, test_preds2))\nprint('-'*50)\n\n\n\n#Confusion matrix\nprint(\"confusion_matrix train is: \", metrics.confusion_matrix(Y_train, train_preds2))\nprint(\"confusion_matrix test is: \", metrics.confusion_matrix(Y_test, test_preds2))\nprint('Wrong predictions out of total')\nprint('-'*50)\n\n# Wrong Predictions made.\nprint((Y_test !=test_preds2).sum(),'/',((Y_test == test_preds2).sum()+(Y_test != test_preds2).sum()))\nprint('-'*50)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:19:45.411863Z","iopub.execute_input":"2021-10-31T18:19:45.412132Z","iopub.status.idle":"2021-10-31T18:20:23.729239Z","shell.execute_reply.started":"2021-10-31T18:19:45.412104Z","shell.execute_reply":"2021-10-31T18:20:23.728432Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Count Vectorized","metadata":{}},{"cell_type":"code","source":"RF=RandomForestClassifier().fit(count_train,Y_train)\n#predict on train \ntrain_preds3 = RF.predict(count_train)\n#accuracy on train\nprint(\"Model accuracy on train is: \", accuracy_score(Y_train, train_preds3))\n\n#predict on test\ntest_preds3 = RF.predict(count_test)\n#accuracy on test\nprint(\"Model accuracy on test is: \", accuracy_score(Y_test, test_preds3))\nprint('-'*50)\n\n\n\n#Confusion matrix\nprint(\"confusion_matrix train is: \", metrics.confusion_matrix(Y_train, train_preds3))\nprint(\"confusion_matrix test is: \", metrics.confusion_matrix(Y_test, test_preds3))\nprint('Wrong predictions out of total')\nprint('-'*50)\n\n# Wrong Predictions made.\nprint((Y_test !=test_preds3).sum(),'/',((Y_test == test_preds3).sum()+(Y_test != test_preds3).sum()))\nprint('-'*50)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:21:00.116984Z","iopub.execute_input":"2021-10-31T18:21:00.117288Z","iopub.status.idle":"2021-10-31T18:21:42.567480Z","shell.execute_reply.started":"2021-10-31T18:21:00.117255Z","shell.execute_reply":"2021-10-31T18:21:42.566570Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# K-Nearest Neighbour","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:21:42.569681Z","iopub.execute_input":"2021-10-31T18:21:42.569991Z","iopub.status.idle":"2021-10-31T18:21:42.575903Z","shell.execute_reply.started":"2021-10-31T18:21:42.569951Z","shell.execute_reply":"2021-10-31T18:21:42.575082Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"#fit the model on train data \nKNN = KNeighborsClassifier().fit(tfidf_train,Y_train)\n#predict on train \ntrain_preds4 = KNN.predict(tfidf_train)\n#accuracy on train\nprint(\"Model accuracy on train is: \", accuracy_score(Y_train, train_preds4))\n\n#predict on test\ntest_preds4 = KNN.predict(tfidf_test)\n#accuracy on test\nprint(\"Model accuracy on test is: \", accuracy_score(Y_test, test_preds4))\nprint('-'*50)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:21:48.357741Z","iopub.execute_input":"2021-10-31T18:21:48.358018Z","iopub.status.idle":"2021-10-31T18:22:16.940283Z","shell.execute_reply.started":"2021-10-31T18:21:48.357988Z","shell.execute_reply":"2021-10-31T18:22:16.939373Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix\nprint(\"confusion_matrix train is: \", metrics.confusion_matrix(Y_train, train_preds4))\nprint(\"confusion_matrix test is: \", metrics.confusion_matrix(Y_test, test_preds4))\nprint('Wrong predictions out of total')\nprint('-'*50)\n\n# Wrong Predictions made.\nprint((Y_test !=test_preds4).sum(),'/',((Y_test == test_preds4).sum()+(Y_test != test_preds4).sum()))\n\nprint('-'*50)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:22:16.941987Z","iopub.execute_input":"2021-10-31T18:22:16.942223Z","iopub.status.idle":"2021-10-31T18:22:16.980142Z","shell.execute_reply.started":"2021-10-31T18:22:16.942193Z","shell.execute_reply":"2021-10-31T18:22:16.979277Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"#fit the model on train data \nKNN = KNeighborsClassifier().fit(count_train,Y_train)\n#predict on train \ntrain_preds5 = KNN.predict(count_train)\n#accuracy on train\nprint(\"Model accuracy on train is: \", accuracy_score(Y_train, train_preds5))\n\n#predict on test\ntest_preds5 = KNN.predict(count_test)\n#accuracy on test\nprint(\"Model accuracy on test is: \", accuracy_score(Y_test, test_preds5))\nprint('-'*50)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:22:16.981612Z","iopub.execute_input":"2021-10-31T18:22:16.982548Z","iopub.status.idle":"2021-10-31T18:22:44.808313Z","shell.execute_reply.started":"2021-10-31T18:22:16.982501Z","shell.execute_reply":"2021-10-31T18:22:44.807245Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix\nprint(\"confusion_matrix train is: \", metrics.confusion_matrix(Y_train, train_preds5))\nprint(\"confusion_matrix test is: \", metrics.confusion_matrix(Y_test, test_preds5))\nprint('Wrong predictions out of total')\nprint('-'*50)\n\n# Wrong Predictions made.\nprint((Y_test !=test_preds5).sum(),'/',((Y_test == test_preds5).sum()+(Y_test != test_preds5).sum()))\n\nprint('-'*50)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:22:44.810211Z","iopub.execute_input":"2021-10-31T18:22:44.810945Z","iopub.status.idle":"2021-10-31T18:22:44.850487Z","shell.execute_reply.started":"2021-10-31T18:22:44.810906Z","shell.execute_reply":"2021-10-31T18:22:44.849835Z"},"trusted":true},"execution_count":57,"outputs":[]}]}